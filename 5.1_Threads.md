# Parallel processing

<!-- https://openjdk.org/jeps/436 -->

## Parallelism

Parallel processing refers to the simultaneous execution of multiple tasks or instructions to solve a problem or perform a computation. It is employed in various fields, such as computer science, engineering, and data analysis, to improve performance, increase efficiency, and solve complex problems.

There are several reasons why parallel processing is used:

1. **Speed and Performance**: By breaking down a task into smaller subtasks and processing them simultaneously, parallel processing can significantly speed up the overall execution time. It allows multiple computations to occur in parallel, harnessing the power of multiple processors or computing resources.

2. **Scalability**: Parallel processing enables systems to handle larger workloads and scale effectively. As data sizes and computational demands increase, parallel processing allows for efficient distribution of tasks across multiple processors or machines, effectively utilizing available resources and avoiding bottlenecks.

3. **Resource Utilization**: With parallel processing, multiple resources can be utilized simultaneously, including CPUs (Central Processing Units), GPUs (Graphics Processing Units), or even distributed computing clusters. This leads to better utilization of hardware resources and increased overall efficiency.

4. **Complex Problem Solving**: Many complex problems can be divided into smaller, more manageable parts that can be solved independently. Parallel processing allows these subproblems to be solved concurrently, accelerating the overall problem-solving process. This is especially useful in fields like scientific research, simulations, and optimization algorithms.

5. **Real-Time Processing**: Certain applications, such as real-time data analysis, signal processing, or video rendering, require rapid processing of continuous streams of data. Parallel processing allows for efficient handling of these time-sensitive tasks by distributing the workload across multiple processors, ensuring timely responses.

It's important to note that not all problems can benefit from parallel processing. Some tasks are inherently sequential and cannot be divided into smaller independent parts. Additionally, parallel processing introduces challenges such as communication overhead, synchronization, and load balancing, which need to be carefully addressed for optimal performance.

### Practical example

It is possible to calculate the equation $y(x) = sin(5x) + cos(9x) + exp(x/3)$ in parallel.
If you have more than one calculation unit available, calculating $sin()$, $cos()$ and $exp()$ in three parallel calculation units speeds up the calculation of the equation $y(x)$.
```mermaid
graph TD
    e1["sin()"] --> r1(y)
    e2["cos()"] --> r1
    e3["exp()"] --> r1
```
It is not always possible to run things in parallel, e.g., the equation $y(x) = sin(5cos(9exp(x/3)))$ cannot run in parallel because terms depend on each other.
```mermaid
graph TD
    e1["exp()"] --> e2("cos()")
    e2          --> e3("sin()")
    e3          --> y
```

### Amdahl's Law

[Amdahl’s law](https://en.wikipedia.org/wiki/Amdahl%27s_law): In parallelization, if $P$ is the proportion of a system or program that can be made parallel, and $1-P$ is the proportion that remains serial, then the maximum speedup $S$that can be achieved using $N$ number of processors is
$$S=1/((1-P)+(P/N))$$

![](images/Amdahl.png)

An example: you have two processors ($N=2$) and all your program can be parallelized ($P=1$), then the speedup is $S=1/(0+½)=2$

Another example: you have four processors ($N=4$), and half of your program can be parallelized ($P=0.5$), then the speedup is $S=1/(0.5+0.5/4)=1.6$

Maximal speed-up can be found by letting the number of processors go to the infinity
$$S_{MAX}=\lim_{n\to\infty}\frac{1}{1-p}$$

### Implementation of Computing

Why we are so much interested about this parallel processing? Due to the advances in silicon process technology, the number of transistors available to construct a processor is increasing all the time.

| Moore's Law                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <center><img src="images/MooreLaw.png"><br></center>[Si atom](https://en.wikipedia.org/wiki/Silicon) size &asymp; 2&Aring; (0.2nm) (A strand of human DNA is 2.5 nm).<br> [Up to 2029 shirking dimensions will do](https://spectrum.ieee.org/a-better-way-to-measure-progress-in-semiconductors), then 3D chips will allow Moore’s phenomenon to continue. Notice that the vertical axis is logarithmic, i.e., the increase of the number of transistors is related to the number of transistors itself - the growth is exponential. |

We are not anymore able to utilize them so easily in our architectures (e.g., by increasing the word length or the number of processor registers).
Therefore we need other ways to ”consume” those new logic components (transistors)
- Larger caches
- More processor cores on the same chip &rarr; concurrency (parallelism)

As a result, every year, desktop processor manufacturers introduce new processor generations that will have larger and larger number of processor cores in a single silicon chip, and larger size of cache memories.

In order to be able to utilize this increasing computational power, we need to be able to easily express parallel computation in our programming languages. In Java (as in most modern programming languages), the tool for expressing parallel computation is _thread_.

## Threads

In the context of computer programming and operating systems, a _process_ refers to an instance of a computer program that is being executed. It represents the execution environment for a program and includes the program code, its current state, memory, and system resources allocated to it by the operating system.

Processes play a crucial role in enabling multitasking, allowing multiple programs or tasks to execute concurrently on a single system. The operating system manages and coordinates these processes, ensuring their efficient execution and resource utilization.

Process's resources (program code, memory, files) are protected by the operating system. This protection gives us reliability and security. For example, in [Android](https://en.wikipedia.org/wiki/Android_(operating_system)) (which runs on top of Linux operating system) every application run on their own processes. If one application crashes, or goes mad, it does not hurt other applications, because the operating system protects their resources (e.g. memory).

While processes provide isolation and concurrency in computing systems, they also present certain challenges. Here are some common problems associated with processes:

1. High Overhead: Processes have significant overhead compared to other execution units, such as threads. Creating and managing processes require system calls, memory allocation, and context switching, which can be resource-intensive and time-consuming. This overhead limits the scalability and efficiency of process-based solutions.

2. Limited Communication and Synchronization: Interprocess communication (IPC) and synchronization between processes can be complex and less efficient than within-thread communication. Processes typically rely on mechanisms like shared memory, message passing, or file-based communication, which involve data copying and serialization, leading to additional overhead and potential synchronization issues.

3. Resource Consumption: Each process consumes its own memory space, which includes code, data, and stack. Having multiple processes running simultaneously can result in significant memory overhead. Additionally, processes may require separate instances of system resources (e.g., file handles), leading to inefficient resource utilization.

4. Context Switching Overhead: When the operating system switches between processes, it performs a context switch, saving the current process state and restoring the state of the next process to be executed. Context switching introduces overhead due to the need to save and restore registers, memory mappings, and other process-specific data.

5. Limited Scalability: Processes are heavyweight units of execution, which can limit the scalability of process-based systems. Creating and managing a large number of processes may lead to resource exhaustion and degraded performance due to the associated overhead and increased context switching.

6. Lack of Shared Memory: Processes typically have separate memory spaces, making it challenging to share data efficiently between them. Interprocess communication often involves data copying or serialization/deserialization, which can be costly in terms of time and resources.

7. Security and Protection: Processes may pose security concerns as they can access and modify their own memory space as well as resources owned by the operating system. Ensuring proper process isolation and protection mechanisms is crucial to prevent unauthorized access or malicious activities.

To address some of these problems, lightweight execution units like _threads_, which share the same resources and memory space within a process, are often employed. Threads provide faster communication and synchronization, reduced overhead, and improved scalability compared to processes. However, processes remain important for achieving strong isolation and security boundaries between different applications or components.

![](images/ProcessVsThread.png)

Note that in modern operating systems, a process contains always at least one thread (of execution). Rule of thumb: process is (mainly) a unit of protection, thread is a unit of execution.

Here are some key points about threads:

1. Thread Execution: Threads are independent sequences of instructions that can be scheduled and executed by the operating system. Each thread has its own program counter, stack, and state, allowing it to run concurrently with other threads within the same process.

2. Concurrency: Multithreading allows multiple threads to execute simultaneously on different processors or CPU cores, if available. This can result in improved performance and responsiveness, as threads can work on different tasks concurrently.

3. Shared Resources: Threads within a process share the same resources, such as memory space, file handles, and other process-related attributes. This enables efficient communication and data sharing between threads, as they can directly access shared variables and data structures.

4. Thread Synchronization: In situations where multiple threads access and modify shared resources concurrently, thread synchronization mechanisms, such as locks, semaphores, or mutexes, are used to ensure proper coordination and prevent race conditions or data inconsistencies.

5. Benefits of Threads: Threads provide several advantages, including increased responsiveness in user interfaces, efficient utilization of CPU resources, improved performance for parallelizable tasks, and simplified program structure by enabling concurrent execution.

It's important to note that threads within a process share the same memory space, which means they can also introduce challenges like data races and synchronization issues. Careful design and synchronization techniques are necessary to ensure proper coordination and avoid conflicts when multiple threads access shared resources.

### Java's Threads

#### Creating Threads

To create a thread of control, you start by creating a `Thread` object
```Java
Thread worker = new Thread();
```
After a `Thread` object is created, you can configure it and then run it. Configuring a thread involves setting its initial priority, name, and so on. When the thread is ready to run, you invoke its `start` method. The `start` method spawns a new thread of control based on the data in the `Thread` object, then returns. Now the Java virtual machine invokes the new thread's `run` method, making the thread active. You can invoke `start` only once for each thread.

When thread's `run` method returns, the thread has exited (stopped and deleted). You can request that a thread cease running by invoking its `interrupt` method - a request well-written thread will always respond to. While a thread is running you can interact with it in other ways, as we will see later.

The standard implementation of `Thread.run` does nothing. To get a thread that does something you must either extend `Thread` to provide a new `run` method or create a `Runnable` object and pass it to the thread's constructor. Let's first discuss how to create new kinds of threads by extending `Thread`. After that we describe how to use `Runnable` in the next section.

Here is a simple two-threaded application that prints the words "ping" and "PONG" at different rates:
```Java
public class PingPong extends Thread {
    private String word;    // what word to print
    private int    delay;   // how long to pause

    public PingPong(String whatToSay, int delayTime) {
        word = whatToSay;
        delay = delayTime;
    }

    public void run() {
        try {
            while (true) {
                System.out.print(word + " ");
                Thread.sleep(delay);    // wait until next time
            }
        } catch (InterruptedException e) {
            return;                     // end this thread
        }
    }

    public static void main(String[] args) {
        new PingPong("ping", 33).start();   // 1/30 second
        new PingPong("PONG", 100).start();  // 1/10 second
    }
}
```
We define a type of thread called `PingPong`. Its run method loops forever, printing its `word` instance variable and sleeping for `delay` milliseconds (`Thread.sleep(dt)` will stop the execution of the current thread of `dt` milliseconds). `PingPong.run` cannot throw exceptions because `Thread.run`, which it overrides, doesn't throw ay exceptions. Accordingly, we must catch the `InterruptedException` that `sleep` can throw (in case its operation is interrupted by the system).

Now we can create some working thread, and `PingPong.main` does just that. It creates two `PingPong` objects, each with its own word and delay cycle, and invokes each thread object's `start` method. Now the threads are off and running. Here is some example output:
```text
ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG
ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG
ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG
ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG
ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG
ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG
ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping
PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping
PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping
PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping
PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping
```
The output contains most of the time a sequence PONG ping ping PONG, but sometimes we get PONG ping PONG sequences. It is a strange thing, because the computer is deterministic machine which should operate always exactly in the same deterministic way. The reason for this strange behaviour is the operating system who schedules those threads (and processes) to be executed, if the operating system is busy to serve other applications, it may be that our `sleep` function delays the time a little more than it was requested, and one ping is delayed over the PONG.

#### Using `Runnable`