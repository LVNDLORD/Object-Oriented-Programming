# Threads

<!-- https://openjdk.org/jeps/436 -->

## Parallelism

Parallel processing refers to the simultaneous execution of multiple tasks or instructions to solve a problem or perform a computation. It is employed in various fields, such as computer science, engineering, and data analysis, to improve performance, increase efficiency, and solve complex problems.

There are several reasons why parallel processing is used:

1. **Speed and Performance**: By breaking down a task into smaller subtasks and processing them simultaneously, parallel processing can significantly speed up the overall execution time. It allows multiple computations to occur in parallel, harnessing the power of multiple processors or computing resources.

2. **Scalability**: Parallel processing enables systems to handle larger workloads and scale effectively. As data sizes and computational demands increase, parallel processing allows for efficient distribution of tasks across multiple processors or machines, effectively utilizing available resources and avoiding bottlenecks.

3. **Resource Utilization**: With parallel processing, multiple resources can be utilized simultaneously, including CPUs (Central Processing Units), GPUs (Graphics Processing Units), or even distributed computing clusters. This leads to better utilization of hardware resources and increased overall efficiency.

4. **Complex Problem Solving**: Many complex problems can be divided into smaller, more manageable parts that can be solved independently. Parallel processing allows these subproblems to be solved concurrently, accelerating the overall problem-solving process. This is especially useful in fields like scientific research, simulations, and optimization algorithms.

5. **Real-Time Processing**: Certain applications, such as real-time data analysis, signal processing, or video rendering, require rapid processing of continuous streams of data. Parallel processing allows for efficient handling of these time-sensitive tasks by distributing the workload across multiple processors, ensuring timely responses.

It's important to note that not all problems can benefit from parallel processing. Some tasks are inherently sequential and cannot be divided into smaller independent parts. Additionally, parallel processing introduces challenges such as communication overhead, synchronization, and load balancing, which need to be carefully addressed for optimal performance.

### Practical example

It is possible to calculate the equation $y(x) = sin(5x) + cos(9x) + exp(x/3)$ in parallel.
If you have more than one calculation unit available, calculating $sin()$, $cos()$ and $exp()$ in three parallel calculation units speeds up the calculation of the equation $y(x)$.

It is not always possible to run things in parallel, e.g., the equation $y(x) = sin(5cos(9exp(x/3)))$ cannot run in parallel because terms depend on each other.

### Amdahl's Law

[Amdahl’s law](https://en.wikipedia.org/wiki/Amdahl%27s_law): In parallelization, if $P$ is the proportion of a system or program that can be made parallel, and $1-P$ is the proportion that remains serial, then the maximum speedup $S$that can be achieved using $N$ number of processors is
$$S=1/((1-P)+(P/N))$$

![](images/Amdahl.png)

An example: you have two processors ($N=2$) and all your program can be parallelized ($P=1$), then the speedup is $1/(0+½)=2$

Another example: you have four processors ($N=4$), and half of your program can be parallelized ($P=0.5$), then the speedup is $1/(0.5+0.5/4)=1.6$

Maximal speed-up can be found by letting the number of processors go to the infinity
$$S_{MAX}=\lim_{n\to\infty}\frac{1}{1-p}$$

### Implementation of Computing

Why we are so much interested about this parallel processing? Due to the advances in silicon process technology, the number of transistors available to construct a processor is increasing all the time.

| Moore's Law                                                                                        |
|----------------------------------------------------------------------------------------------------|
| <center><img src="images/MooreLaw.png"><br></center>[Si atom](https://en.wikipedia.org/wiki/Silicon) size &asymp; 2&Aring; (0,2nm) (A strand of human DNA is 2.5 nm).<br> [Up to 2029 shirking dimensions will do](https://spectrum.ieee.org/a-better-way-to-measure-progress-in-semiconductors), then 3D chips will allow Moore’s phenomenon to continue. |

We are not able to utilize them anymore so easily in our architectures (e.g., increasing word length or the number of processor registers).
Therefore we need other ways to ”consume” those new logic components (transistors)
- Larger caches
- More processor cores on the same chip &rarr; concurrency (parallelism)

As a result, every year, desktop processor manufacturers introduce new processor generations that will have larger number of processor cores in a single silicon chip, and larger size of cache memories.

## Java's Threads

