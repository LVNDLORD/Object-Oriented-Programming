# Parallel processing

<!-- https://openjdk.org/jeps/436 -->

## Parallelism

Parallel processing refers to the simultaneous execution of multiple tasks or instructions to solve a problem or perform a computation. It is employed in various fields, such as computer science, engineering, and data analysis, to improve performance, increase efficiency, and solve complex problems.

There are several reasons why parallel processing is used:

1. **Speed and Performance**: By breaking down a task into smaller subtasks and processing them simultaneously, parallel processing can significantly speed up the overall execution time. It allows multiple computations to occur in parallel, harnessing the power of multiple processors or computing resources.

2. **Scalability**: Parallel processing enables systems to handle larger workloads and scale effectively. As data sizes and computational demands increase, parallel processing allows for efficient distribution of tasks across multiple processors or machines, effectively utilizing available resources and avoiding bottlenecks.

3. **Resource Utilization**: With parallel processing, multiple resources can be utilized simultaneously, including CPUs (Central Processing Units), GPUs (Graphics Processing Units), or even distributed computing clusters. This leads to better utilization of hardware resources and increased overall efficiency.

4. **Complex Problem Solving**: Many complex problems can be divided into smaller, more manageable parts that can be solved independently. Parallel processing allows these subproblems to be solved concurrently, accelerating the overall problem-solving process. This is especially useful in fields like scientific research, simulations, and optimization algorithms.

5. **Real-Time Processing**: Certain applications, such as real-time data analysis, signal processing, or video rendering, require rapid processing of continuous streams of data. Parallel processing allows for efficient handling of these time-sensitive tasks by distributing the workload across multiple processors, ensuring timely responses.

It's important to note that not all problems can benefit from parallel processing. Some tasks are inherently sequential and cannot be divided into smaller independent parts. Additionally, parallel processing introduces challenges such as communication overhead, synchronization, and load balancing, which need to be carefully addressed for optimal performance.

### Practical example

It is possible to calculate the equation $y(x) = sin(5x) + cos(9x) + exp(x/3)$ in parallel.
If you have more than one calculation unit available, calculating $sin()$, $cos()$ and $exp()$ in three parallel calculation units speeds up the calculation of the equation $y(x)$.
```mermaid
graph TD
    e1["sin()"] --> r1(y)
    e2["cos()"] --> r1
    e3["exp()"] --> r1
```
It is not always possible to run things in parallel, e.g., the equation $y(x) = sin(5cos(9exp(x/3)))$ cannot run in parallel because terms depend on each other.
```mermaid
graph TD
    e1["exp()"] --> e2("cos()")
    e2          --> e3("sin()")
    e3          --> y
```

### Amdahl's Law

[Amdahl’s law](https://en.wikipedia.org/wiki/Amdahl%27s_law): In parallelization, if $P$ is the proportion of a system or program that can be made parallel, and $1-P$ is the proportion that remains serial, then the maximum speedup $S$that can be achieved using $N$ number of processors is
$$S=1/((1-P)+(P/N))$$

![](images/Amdahl.png)

An example: you have two processors ($N=2$) and all your program can be parallelized ($P=1$), then the speedup is $S=1/(0+½)=2$

Another example: you have four processors ($N=4$), and half of your program can be parallelized ($P=0.5$), then the speedup is $S=1/(0.5+0.5/4)=1.6$

Maximal speed-up can be found by letting the number of processors go to the infinity
$$S_{MAX}=\lim_{n\to\infty}\frac{1}{1-p}$$

### Implementation of Computing

Why we are so much interested about this parallel processing? Due to the advances in silicon process technology, the number of transistors available to construct a processor is increasing all the time.

| Moore's Law                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| <center><img src="images/MooreLaw.png"><br></center>[Si atom](https://en.wikipedia.org/wiki/Silicon) size &asymp; 2&Aring; (0.2nm) (A strand of human DNA is 2.5 nm).<br> [Up to 2029 shirking dimensions will do](https://spectrum.ieee.org/a-better-way-to-measure-progress-in-semiconductors), then 3D chips will allow Moore’s phenomenon to continue. Notice that the vertical axis is logarithmic, i.e., the increase of the number of transistors is related to the number of transistors itself - the growth is exponential. |

We are not anymore able to utilize them so easily in our architectures (e.g., by increasing the word length or the number of processor registers).
Therefore we need other ways to ”consume” those new logic components (transistors)
- Larger caches
- More processor cores on the same chip &rarr; concurrency (parallelism)

As a result, every year, desktop processor manufacturers introduce new processor generations that will have larger and larger number of processor cores in a single silicon chip, and larger size of cache memories.

In order to be able to utilize this increasing computational power, we need to be able to easily express parallel computation in our programming languages. In Java (as in most modern programming languages), the tool for expressing parallel computation is _thread_.

## Threads

In the context of computer programming and operating systems, a thread refers to a lightweight unit of execution within a process. Threads are a fundamental component of multithreading, a technique where multiple threads within a single process can execute concurrently, sharing the same resources and memory space.

Here are some key points about threads:

1. Thread Execution: Threads are independent sequences of instructions that can be scheduled and executed by the operating system. Each thread has its own program counter, stack, and state, allowing it to run concurrently with other threads within the same process.

2. Concurrency: Multithreading allows multiple threads to execute simultaneously on different processors or CPU cores, if available. This can result in improved performance and responsiveness, as threads can work on different tasks concurrently.

3. Shared Resources: Threads within a process share the same resources, such as memory space, file handles, and other process-related attributes. This enables efficient communication and data sharing between threads, as they can directly access shared variables and data structures.

4. Thread Synchronization: In situations where multiple threads access and modify shared resources concurrently, thread synchronization mechanisms, such as locks, semaphores, or mutexes, are used to ensure proper coordination and prevent race conditions or data inconsistencies.

5. Benefits of Threads: Threads provide several advantages, including increased responsiveness in user interfaces, efficient utilization of CPU resources, improved performance for parallelizable tasks, and simplified program structure by enabling concurrent execution.

It's important to note that threads within a process share the same memory space, which means they can also introduce challenges like data races and synchronization issues. Careful design and synchronization techniques are necessary to ensure proper coordination and avoid conflicts when multiple threads access shared resources.

### Java's Threads

```Java
public class PingPong extends Thread {
    private String word;    // what word to print
    private int    delay;   // how long to pause

    public PingPong(String whatToSay, int delayTime) {
        word = whatToSay;
        delay = delayTime;
    }

    public void run() {
        try {
            while (true) {
                System.out.print(word + " ");
                Thread.sleep(delay);    // wait until next time
            }
        } catch (InterruptedException e) {
            return;                     // end this thread
        }
    }

    public static void main(String[] args) {
        new PingPong("ping", 33).start();   // 1/30 second
        new PingPong("PONG", 100).start();  // 1/10 second
    }
}
```

```text
ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping PONG ping ping PONG ping ping PONG ping ping PONG ping ping PONG ping
```

